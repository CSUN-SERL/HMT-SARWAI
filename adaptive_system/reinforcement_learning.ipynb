{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reinforcement_learning import EpsilonGreedy\n",
    "from reinforcement_learning import NeuralNetwork\n",
    "from reinforcement_learning import ReplayMemory\n",
    "from reinforcement_learning import LinearControlSignal\n",
    "\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.n_queries = 0\n",
    "    \n",
    "    def getState(self):\n",
    "        return np.array([[self.n_queries]])\n",
    "    \n",
    "    def calculateReward(self, action):\n",
    "        if action == 0 and self.n_queries > 5:\n",
    "            return 0.8\n",
    "        elif action == 0 and self.n_queries <= 5:\n",
    "            return -0.8\n",
    "        elif action == 1 and self.n_queries > 5:\n",
    "            return -0.8\n",
    "        elif action == 1 and self.n_queries <= 5:\n",
    "            return 0.8\n",
    "            \n",
    "    \n",
    "    def step(self, action):\n",
    "        # More queries show up.\n",
    "        self.n_queries += random.randint(0, 5)\n",
    "        \n",
    "        # User answers some.\n",
    "        self.n_queries -= random.randint(0, self.n_queries)\n",
    "        \n",
    "        state = np.array([[self.n_queries]])\n",
    "        \n",
    "        reward = self.calculateReward(action)\n",
    "        \n",
    "        # Placeholder\n",
    "        end_episode = 0\n",
    "        \n",
    "        info = ''\n",
    "        \n",
    "        return [state, reward, end_episode, info]\n",
    "    \n",
    "    def get_action_meanings(self):\n",
    "        return ['Fully Autonomous', 'Not Autonomous']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to restore last checkpoint ...\n",
      "('Failed to restore checkpoint from:', None)\n",
      "Initializing variables instead.\n"
     ]
    }
   ],
   "source": [
    "# Create the game-environment using OpenAI Gym.\n",
    "env = Environment()\n",
    "\n",
    "# The number of possible actions that the agent may take in every step.\n",
    "num_actions = 2\n",
    "\n",
    "# Whether we are training (True) or testing (False).\n",
    "training = True\n",
    "\n",
    "# Whether to use logging during training.\n",
    "use_logging = False\n",
    "\n",
    "if use_logging and training:\n",
    "    # Used for logging Q-values and rewards during training.\n",
    "    log_q_values = LogQValues()\n",
    "    log_reward = LogReward()\n",
    "else:\n",
    "    log_q_values = None\n",
    "    log_reward = None\n",
    "\n",
    "# List of string-names for the actions in the game-environment.\n",
    "action_names = env.get_action_meanings()\n",
    "\n",
    "# Epsilon-greedy policy for selecting an action from the Q-values.\n",
    "# During training the epsilon is decreased linearly over the given\n",
    "# number of iterations. During testing the fixed epsilon is used.\n",
    "epsilon_greedy = EpsilonGreedy(start_value=1.0,\n",
    "                                    end_value=0.1,\n",
    "                                    num_iterations=1e6,\n",
    "                                    num_actions=num_actions,\n",
    "                                    epsilon_testing=0.01)\n",
    "\n",
    "if training:\n",
    "    # The following control-signals are only used during training.\n",
    "\n",
    "    # The learning-rate for the optimizer decreases linearly.\n",
    "    learning_rate_control = LinearControlSignal(start_value=1e-3,\n",
    "                                                     end_value=1e-5,\n",
    "                                                     num_iterations=5e6)\n",
    "\n",
    "    # The loss-limit is used to abort the optimization whenever the\n",
    "    # mean batch-loss falls below this limit.\n",
    "    loss_limit_control = LinearControlSignal(start_value=0.1,\n",
    "                                                  end_value=0.015,\n",
    "                                                  num_iterations=5e6)\n",
    "\n",
    "    # The maximum number of epochs to perform during optimization.\n",
    "    # This is increased from 5 to 10 epochs, because it was found for\n",
    "    # the Breakout-game that too many epochs could be harmful early\n",
    "    # in the training, as it might cause over-fitting.\n",
    "    # Later in the training we would occasionally get rare events\n",
    "    # and would therefore have to optimize for more iterations\n",
    "    # because the learning-rate had been decreased.\n",
    "    max_epochs_control = LinearControlSignal(start_value=5.0,\n",
    "                                                  end_value=10.0,\n",
    "                                                  num_iterations=5e6)\n",
    "\n",
    "    # The fraction of the replay-memory to be used.\n",
    "    # Early in the training, we want to optimize more frequently\n",
    "    # so the Neural Network is trained faster and the Q-values\n",
    "    # are learned and updated more often. Later in the training,\n",
    "    # we need more samples in the replay-memory to have sufficient\n",
    "    # diversity, otherwise the Neural Network will over-fit.\n",
    "    replay_fraction = LinearControlSignal(start_value=0.1,\n",
    "                                               end_value=1.0,\n",
    "                                               num_iterations=5e6)\n",
    "else:\n",
    "    # We set these objects to None when they will not be used.\n",
    "    learning_rate_control = None\n",
    "    loss_limit_control = None\n",
    "    max_epochs_control = None\n",
    "    replay_fraction = None\n",
    "\n",
    "if training:\n",
    "    # We only create the replay-memory when we are training the agent,\n",
    "    # because it requires a lot of RAM. The image-frames from the\n",
    "    # game-environment are resized to 105 x 80 pixels gray-scale,\n",
    "    # and each state has 2 channels (one for the recent image-frame\n",
    "    # of the game-environment, and one for the motion-trace).\n",
    "    # Each pixel is 1 byte, so this replay-memory needs more than\n",
    "    # 3 GB RAM (105 x 80 x 2 x 200000 bytes).\n",
    "\n",
    "    replay_memory = ReplayMemory(size=200000,\n",
    "                                num_actions=num_actions)\n",
    "else:\n",
    "    replay_memory = None\n",
    "\n",
    "# Create the Neural Network used for estimating Q-values.\n",
    "model = NeuralNetwork(num_actions=num_actions,\n",
    "                      replay_memory=replay_memory,\n",
    "                      checkpoint_dir='checkpoints',\n",
    "                      use_pretty_tensor=False, state_shape = [1])\n",
    "\n",
    "# Log of the rewards obtained in each episode during calls to run()\n",
    "episode_rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_q_values(env.getState())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.getState()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, reward, end, info = env.step(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00053891,  0.00037644]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_q_values(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
